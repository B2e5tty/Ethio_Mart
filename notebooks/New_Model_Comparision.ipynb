{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhUuMX7uFef-"
      },
      "source": [
        "# Task 4:  Model Comparison & Selection\n",
        "**Objective**\n",
        "Compare different models and select the best-performing one for the entity extraction task.\n",
        "\n",
        "**Steps**\n",
        "\n",
        "1. Finetune multiple models like XLM-Roberta: A large multilingual model for NER tasks, or DistilBERT: A smaller, lighter model for more efficient NER tasks, or mBERT (Multilingual BERT): A multilingual version of BERT, suitable for Amharic or .others?\n",
        "2. Evaluate the fine-tuned model on the validation set to check performance.\n",
        "3. Compare models based on accuracy, speed, and robustness in handling multi-modal data.\n",
        "4. Select the best-performing model for production based on evaluation metrics.\n",
        "\n",
        "# Task 5: Model Interpretability\n",
        "**Objective**\n",
        "Use model interpretability tools to explain how the NER model identifies entities, ensuring transparency and trust in the system.\n",
        "\n",
        "**Steps:**\n",
        "1. Implement SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) to interpret the modelâ€™s predictions.\n",
        "2. Analyze difficult cases where the model might struggle to identify entities correctly (e.g., ambiguous text, overlapping entities).\n",
        "3. Generate reports on how the model makes decisions and identify areas for improvement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "M96e5WzbD_qI",
        "outputId": "9a0b1945-5827-4a7c-b8c1-308ef70986a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Collecting slicer==0.0.8 (from shap)\n",
            "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Downloading shap-0.46.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (540 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m540.1/540.1 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.46.0 slicer-0.0.8\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.5.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.24.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=618b1099a3002c16d8c7e31ce81379b5374a0f3dae324af360d1b151dbe42808\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/a2/af/9ac0a1a85a27f314a06b39e1f492bee1547d52549a4606ed89\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets\n",
        "pip install seqeval\n",
        "pip install shap\n",
        "pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0v0JLLLjEA2i"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
        "from seqeval.metrics import classification_report as seq_classification_report\n",
        "# from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PxhSG4BPGzNh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_CkEIW4EBew",
        "outputId": "921e6ac0-4688-4357-b185-784ea914e5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "file_path = '/content/drive/My Drive/tokens_labels.conll'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    contents = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XxpwvEH3EBvj"
      },
      "outputs": [],
      "source": [
        "# extract tokens and labels from the dataset\n",
        "def extract_tokens_labels(text):\n",
        "  words = []\n",
        "  labels = []\n",
        "  # checks for English word\n",
        "  def is_amharic(word):\n",
        "      # Amharic characters are in the Unicode range: 1200-137F (hex)\n",
        "      for char in word:\n",
        "          if not (0x1200 <= ord(char) <= 0x137F):\n",
        "              return False\n",
        "      return True\n",
        "\n",
        "  # split tokens and labels\n",
        "  for con in content:\n",
        "    con = con.strip().replace('[', '').replace(']', '').replace(',', '').replace(\"'\", \"\").split(' ')\n",
        "    if not(is_amharic(con[0])):\n",
        "      pass\n",
        "    else:\n",
        "      words.append(con[0])\n",
        "      labels.append(con[-1])\n",
        "\n",
        "  return words, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "F4xMS-tXECAu"
      },
      "outputs": [],
      "source": [
        "# align tokens and labels\n",
        "def align_token_label(text, tokenizer):\n",
        "  # labels to id number\n",
        "  label_to_id = {\n",
        "      \"O\": 0,\n",
        "      \"B-LOC\": 1,\n",
        "      \"I-LOC\": 2,\n",
        "      \"B-PRODUCT\": 3,\n",
        "      \"I-PRODUCT\": 4,\n",
        "      \"B-PRICE\": 5,\n",
        "      \"I-PRICE\": 6\n",
        "}\n",
        "  tokens, labels = extract_tokens_labels(text)\n",
        "  tokenized_inputs = tokenizer(tokens, truncation = True, padding = True, is_split_into_words = True)\n",
        "\n",
        "  word_ids = tokenized_inputs.word_ids()\n",
        "  aligned_labels = []\n",
        "\n",
        "  previous_id = None\n",
        "  for k,id in enumerate(word_ids):\n",
        "    if id is None:\n",
        "      aligned_labels.append(-100)\n",
        "\n",
        "    elif id != previous_id:\n",
        "      aligned_labels.append(label_to_id[labels[id]])\n",
        "\n",
        "    else:\n",
        "      aligned_labels.append(-100)\n",
        "\n",
        "    previous_id = id\n",
        "  tokenized_inputs['labels'] = aligned_labels\n",
        "  # print(aligned_labels)\n",
        "  return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pLD_8xkun5UK"
      },
      "outputs": [],
      "source": [
        "# SHAP report generation\n",
        "def generate_shap_report(model, tokenizer, validation_data):\n",
        "    explainer = shap.KernelExplainer(lambda x: model(**tokenizer(x, return_tensors=\"pt\", truncation=True, padding=True)).logits.detach().numpy(),\n",
        "                                     shap.sample(validation_data, 100))  # Sample for efficiency\n",
        "\n",
        "    # Example sentence for SHAP explanation\n",
        "    test_sentence = \"áŠ á•áˆ áŠ á‹²áˆ±áŠ• áŠ á‹­áŽáŠ• á‰ áŠ’á‹á‹®áˆ­áŠ­ áŠ¨á‰°áˆ› á‰ 4000á‹‹áŒ‹ áŠ¥á‹¨áˆˆá‰€á‰€á‰½ áŠá‹á¢\"\n",
        "    shap_values = explainer.shap_values([test_sentence])\n",
        "\n",
        "    # Visualize SHAP results\n",
        "    shap.initjs()\n",
        "    shap.force_plot(explainer.expected_value, shap_values[0], test_sentence.split())\n",
        "\n",
        "  # LIME report generation\n",
        "def generate_lime_report(model, tokenizer, validation_data):\n",
        "    explainer = LimeTextExplainer(class_names=[\"O\", \"B-LOC\", \"I-LOC\", \"B-PRODUCT\", \"I-PRODUCT\", \"B-PRICE\", \"I-PRICE\"])\n",
        "\n",
        "    def lime_predict(texts):\n",
        "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        outputs = model(**inputs)\n",
        "        predictions = outputs.logits.detach().numpy()\n",
        "        return predictions\n",
        "\n",
        "    # Example sentence for LIME explanation\n",
        "    test_sentence = \"áŠ á‹²áˆ± áŒ‹áˆ‹áŠ­áˆ² áˆµáˆáŠ©áŠ• áˆ³áˆáˆ°áŠ•áŒ á‰ áŠ á‹²áˆµ áŠ á‰ á‰£ á‰ 3000á‹‹áŒ‹ áŠ áˆµáˆ˜áˆ¨á‰€á‰½á¢\"\n",
        "    exp = explainer.explain_instance(test_sentence, lime_predict, num_features=10)\n",
        "\n",
        "    # Show LIME explanation\n",
        "    exp.show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Dw4zvrZ_ENV0"
      },
      "outputs": [],
      "source": [
        "# take a subset\n",
        "content = contents[0:1000]\n",
        "\n",
        "# split validation and train sets\n",
        "train_data, validation_data = train_test_split(content, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "fQeI44c1EOgF"
      },
      "outputs": [],
      "source": [
        "def different_model(model,train_data,validation_data):\n",
        "  # intializing model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  model = AutoModelForTokenClassification.from_pretrained(model,num_labels=7)\n",
        "\n",
        "  # Create dictionaries to hold the tokenized datasets\n",
        "  tokenized_datasets = {'train': [], 'validation': []}\n",
        "\n",
        "  batch_size = 4\n",
        "  # batch for faster computation\n",
        "  def batch_data(data, batch_size):\n",
        "    for i in range(0, len(data), batch_size):\n",
        "      yield data[i:i + batch_size]\n",
        "\n",
        "  # align_token_label for train dataset\n",
        "  for batch in batch_data(train_data,batch_size):\n",
        "    tokenized_batch = [align_token_label(con, tokenizer) for con in batch]\n",
        "    tokenized_datasets['train'].extend(tokenized_batch)\n",
        "\n",
        "  # align_token_label for validation dataset\n",
        "  for batch in batch_data(validation_data,batch_size):\n",
        "      tokenized_batch = [align_token_label(con, tokenizer) for con in batch]\n",
        "      tokenized_datasets['validation'].extend(tokenized_batch)\n",
        "\n",
        "  # Convert lists to Hugging Face Dataset objects\n",
        "  tokenized_datasets['train'] = Dataset.from_list(tokenized_datasets['train'])\n",
        "  tokenized_datasets['validation'] = Dataset.from_list(tokenized_datasets['validation'])\n",
        "  print('e')\n",
        "  # fine tunning the model\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir = '/content/drive/My Drive/results',\n",
        "      evaluation_strategy = 'epoch',\n",
        "      learning_rate = 2e-5,\n",
        "      per_device_train_batch_size = 4,\n",
        "      per_device_eval_batch_size = 4,\n",
        "      gradient_accumulation_steps = 4,\n",
        "      num_train_epochs = 3,\n",
        "      weight_decay = 0.01,\n",
        "      fp16 = True # Enable mixed precision training\n",
        "      #  no_cuda=True  # Force CPU training\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model = model,\n",
        "      args = training_args,\n",
        "      train_dataset = tokenized_datasets['train'],\n",
        "      eval_dataset = tokenized_datasets['validation'],\n",
        "  )\n",
        "  # train the model\n",
        "  trainer.train()\n",
        "\n",
        "  # evaluate the model\n",
        "  print(trainer.evaluate())\n",
        "\n",
        "    # Get predictions from the model\n",
        "  predictions, labels, _ = trainer.predict(tokenized_datasets['validation'])\n",
        "\n",
        "  # Convert predictions to label ids\n",
        "  predicted_label_ids = np.argmax(predictions, axis=-1)\n",
        "\n",
        "  label_names = {\n",
        "      0: \"O\",\n",
        "      1: \"B-LOC\",\n",
        "      2: \"I-LOC\",\n",
        "      3: \"B-PRODUCT\",\n",
        "      4: \"I-PRODUCT\",\n",
        "      5: \"B-PRICE\",\n",
        "      6: \"I-PRICE\"\n",
        "  }\n",
        "\n",
        "  # Convert the label ids to actual label names\n",
        "  true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "  predicted_labels = [[label_names[p] for p, l in zip(pred, label) if l != -100]\n",
        "                      for pred, label in zip(predicted_label_ids, labels)]\n",
        "\n",
        "  # Use sklearn to compute the classification report\n",
        "  print(seq_classification_report(true_labels, predicted_labels))\n",
        "\n",
        "  if model == 'bert-base-multilingual-cased':       # wrote this after comparing the models\n",
        "        generate_shap_report(model, tokenizer, tokenized_datasets['validation'])\n",
        "        generate_lime_report(model, tokenizer, tokenized_datasets['validation'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OFY8Xp8VEPX-",
        "outputId": "53d5f9b9-797a-4ffb-d7b3-7fac670ac758"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 03:13, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.020477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.001157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.0007777191931381822, 'eval_runtime': 5.186, 'eval_samples_per_second': 38.566, 'eval_steps_per_second': 9.641, 'epoch': 3.0}\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       1.00      1.00      1.00      1600\n",
            "       PRICE       1.00      1.00      1.00       200\n",
            "     PRODUCT       1.00      1.00      1.00      1200\n",
            "\n",
            "   micro avg       1.00      1.00      1.00      3000\n",
            "   macro avg       1.00      1.00      1.00      3000\n",
            "weighted avg       1.00      1.00      1.00      3000\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 01:51, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.384748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.192141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.107240</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.10723968595266342, 'eval_runtime': 2.8967, 'eval_samples_per_second': 69.043, 'eval_steps_per_second': 17.261, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.79      0.68      0.73      4400\n",
            "       PRICE       0.00      0.00      0.00       800\n",
            "     PRODUCT       0.20      0.08      0.11      2600\n",
            "\n",
            "   micro avg       0.67      0.41      0.51      7800\n",
            "   macro avg       0.33      0.25      0.28      7800\n",
            "weighted avg       0.51      0.41      0.45      7800\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "e\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 02:07, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.325100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.196404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.141046</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.14104604721069336, 'eval_runtime': 2.9881, 'eval_samples_per_second': 66.932, 'eval_steps_per_second': 16.733, 'epoch': 3.0}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.65      0.50      0.56      4400\n",
            "       PRICE       0.00      0.00      0.00       800\n",
            "     PRODUCT       0.67      0.31      0.42      2600\n",
            "\n",
            "   micro avg       0.65      0.38      0.48      7800\n",
            "   macro avg       0.44      0.27      0.33      7800\n",
            "weighted avg       0.59      0.38      0.46      7800\n",
            "\n"
          ]
        }
      ],
      "source": [
        "model_name = ['xlm-roberta-base','distilbert-base-multilingual-cased','bert-base-multilingual-cased']\n",
        "for model in model_name:\n",
        "  different_model(model,train_data,validation_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx6MoI0bEcWa"
      },
      "outputs": [],
      "source": [
        "# # for cleaning up memory\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCcNK6wslFcJ"
      },
      "source": [
        "Model **bert-base-multilingual-cased** is a better option than Model **distilbert-base-multilingual-cased** because it has stronger performance on both LOC and PRODUCT, while Model **xlm-roberta-base** struggles significantly with PRODUCT.\n",
        "\n",
        "Model **distilbert-base-multilingual-cased** shows perfect scores in all metrics, but this might indicate overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKmdOW0-mClq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88xY960OmCYt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joxIZCxmmCLx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49CTNf0qmB6U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
