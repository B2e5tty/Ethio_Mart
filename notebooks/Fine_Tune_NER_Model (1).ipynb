{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1pvajwkE0hg"
      },
      "source": [
        "# Task 3: Fine Tune NER Model\n",
        "**Objective**\n",
        "\n",
        "Fine-tune a Named Entity Recognition (NER) model to extract key entities (e.g., products, prices, and location) from Amharic Telegram messages.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. Use Google Colab or any other environment with GPU support for faster training.\n",
        "2. Install necessary libraries by running the following commands:\n",
        "3. You will use the pre-trained XLM-Roberta or bert-tiny-amharic or afroxmlr model, which supports multilingual tasks, including Amharic.\n",
        "4. Load the labeled dataset in CoNLL format from the previous task.\n",
        "5. You can use Hugging Face's datasets library to load the data or manually parse the CoNLL format into a pandas DataFrame.\n",
        "6. Tokenize the data and align the labels with tokens produced by the tokenizer\n",
        "7. Set up training arguments, such as learning rate, number of epochs, batch size, and evaluation strategy.\n",
        "8. Use Hugging Face's Trainer API to fine-tune the model.\n",
        "9. Evaluate the fine-tuned model on the validation set to check performance.\n",
        "10. After fine-tuning, save the model for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xzpFQlYnocV5",
        "outputId": "dbec1f26-1dba-492a-f7fe-40ca281e9adf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osgTVkqoBagn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, Features, Sequence, ClassLabel, Value\n",
        "from sklearn.metrics import classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNyW9FJgpvI3",
        "outputId": "88572ca6-ca1f-4ac8-f2d1-f4772b888fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/tokens_labels.conll'\n",
        "\n",
        "with open(file_path, 'r') as file:\n",
        "    contents = file.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NOfcP-hAjY1O"
      },
      "outputs": [],
      "source": [
        "# extract tokens and labels from the dataset\n",
        "def extract_tokens_labels(text):\n",
        "  words = []\n",
        "  labels = []\n",
        "  # checks for English word\n",
        "  def is_amharic(word):\n",
        "      # Amharic characters are in the Unicode range: 1200-137F (hex)\n",
        "      for char in word:\n",
        "          if not (0x1200 <= ord(char) <= 0x137F):\n",
        "              return False\n",
        "      return True\n",
        "\n",
        "  # split tokens and labels\n",
        "  for con in content:\n",
        "    con = con.strip().replace('[', '').replace(']', '').replace(',', '').replace(\"'\", \"\").split(' ')\n",
        "    if not(is_amharic(con[0])):\n",
        "      pass\n",
        "    else:\n",
        "      words.append(con[0])\n",
        "      labels.append(con[-1])\n",
        "\n",
        "  return words, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "95TIqFAFU1i8"
      },
      "outputs": [],
      "source": [
        "# align tokens and labels\n",
        "def align_token_label(text, tokenizer):\n",
        "  # labels to id number\n",
        "  label_to_id = {\n",
        "      \"O\": 0,\n",
        "      \"B-LOC\": 1,\n",
        "      \"I-LOC\": 2,\n",
        "      \"B-PRODUCT\": 3,\n",
        "      \"I-PRODUCT\": 4,\n",
        "      \"B-PRICE\": 5,\n",
        "      \"I-PRICE\": 6\n",
        "}\n",
        "  tokens, labels = extract_tokens_labels(text)\n",
        "  tokenized_inputs = tokenizer(tokens, truncation = True, padding = True, is_split_into_words = True)\n",
        "\n",
        "  word_ids = tokenized_inputs.word_ids()\n",
        "  aligned_labels = []\n",
        "\n",
        "  previous_id = None\n",
        "  for k,id in enumerate(word_ids):\n",
        "    if id is None:\n",
        "      aligned_labels.append(-100)\n",
        "\n",
        "    elif id != previous_id:\n",
        "      aligned_labels.append(label_to_id[labels[id]])\n",
        "\n",
        "    else:\n",
        "      aligned_labels.append(-100)\n",
        "\n",
        "    previous_id = id\n",
        "  tokenized_inputs['labels'] = aligned_labels\n",
        "  # print(aligned_labels)\n",
        "  return tokenized_inputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f2VDyBlFpvVu",
        "outputId": "43141ffe-e658-45b4-ea7f-d25e22eeb699"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# take a subset\n",
        "content = contents[0:1000]\n",
        "\n",
        "# split validation and train sets\n",
        "train_data, validation_data = train_test_split(content, test_size=0.2, random_state=42)\n",
        "\n",
        "# intializing model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"masakhane/afroxlmr-large-ner-masakhaner-1.0_2.0\",gradient_checkpointing=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "collapsed": true,
        "id": "EfpL8h48_CZc"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries to hold the tokenized datasets\n",
        "tokenized_datasets = {'train': [], 'validation': []}\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# batch for faster computation\n",
        "def batch_data(data, batch_size):\n",
        "  for i in range(0, len(data), batch_size):\n",
        "    yield data[i:i + batch_size]\n",
        "\n",
        "# align_token_label for train dataset\n",
        "for batch in batch_data(train_data,batch_size):\n",
        "  tokenized_batch = [align_token_label(con, tokenizer) for con in batch]\n",
        "  tokenized_datasets['train'].extend(tokenized_batch)\n",
        "\n",
        "# align_token_label for validation dataset\n",
        "for batch in batch_data(validation_data,batch_size):\n",
        "    tokenized_batch = [align_token_label(con, tokenizer) for con in batch]\n",
        "    tokenized_datasets['validation'].extend(tokenized_batch)\n",
        "\n",
        "# Convert lists to Hugging Face Dataset objects\n",
        "tokenized_datasets['train'] = Dataset.from_list(tokenized_datasets['train'])\n",
        "tokenized_datasets['validation'] = Dataset.from_list(tokenized_datasets['validation'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "XaeuGYs_D5Pm",
        "outputId": "be14f7c7-c25f-45c9-cff0-f8b3e87fb5c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
            "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [150/150 08:34, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000008</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:08]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 8.087497917586006e-06,\n",
              " 'eval_runtime': 8.4695,\n",
              " 'eval_samples_per_second': 23.614,\n",
              " 'eval_steps_per_second': 5.904,\n",
              " 'epoch': 3.0}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# fine tunning the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = '/content/drive/My Drive/results',\n",
        "    evaluation_strategy = 'epoch',\n",
        "    learning_rate = 2e-5,\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size = 4,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    num_train_epochs = 3,\n",
        "    weight_decay = 0.01,\n",
        "    fp16 = True # Enable mixed precision training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['validation'],\n",
        ")\n",
        "\n",
        "# train the model\n",
        "trainer.train()\n",
        "\n",
        "# evaluate the model\n",
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN6PzxAoXhyk",
        "outputId": "8399077f-355a-4218-ec2d-7c85fc04518a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('afroxlmr_fine_tuned_model/tokenizer_config.json',\n",
              " 'afroxlmr_fine_tuned_model/special_tokens_map.json',\n",
              " 'afroxlmr_fine_tuned_model/sentencepiece.bpe.model',\n",
              " 'afroxlmr_fine_tuned_model/added_tokens.json',\n",
              " 'afroxlmr_fine_tuned_model/tokenizer.json')"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"afroxlmr_fine_tuned_model\")\n",
        "tokenizer.save_pretrained(\"afroxlmr_fine_tuned_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "BPBO3wiXSB7-"
      },
      "outputs": [],
      "source": [
        "# for cleaning up memory\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wViSejkp_Dh6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0WEv9zO_Dv-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2nj2Kek_D81"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
